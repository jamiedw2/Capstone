---
title: "Data Science Capstone Project: Milestone report"
author: "James Whitehead"
date: "18 February 2017"
output: html_document
---

```{r, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(stringi)
library(SnowballC)
setwd("~/Coursera/Capstone")
unzip("Coursera-SwiftKey.zip",
      c("final/en_US/en_US.blogs.txt",
        "final/en_US/en_US.news.txt",
        "final/en_US/en_US.twitter.txt"))
#load in using readLines
blogs <- readLines("final/en_US/en_US.blogs.txt")
news <- readLines("final/en_US/en_US.news.txt")
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)

profanURL="http://www.bannedwordlist.com/lists/swearWords.txt"
download.file(profanURL, "profanList.txt")
profanList <- read.table("profanList.txt")
```

##Basic data summary

The dataset contains large amounts of text samples from 3 documents (blogs, news and twitter), and were originally available from http://www.corpora.heliohost.org. The task of the capstone project is to use these texts as training data to build a predictive text model, such as might be used on a mobile device. This report aims to show some basic exploratory data analysis. The code chunks used for this report have been hidden, but may be viewed in the markdown document available at https://github.com/jamiedw2/Capstone.

Below is a table showing the size (in MB), line counts and word counts for each of the documents.

```{r, cache=TRUE, echo=FALSE}
data.frame(Document=c("Blogs","News","Twitter"),
           Filesize=c(file.size("final/en_US/en_US.blogs.txt")/(1024^2),
                      file.size("final/en_US/en_US.news.txt")/(1024^2),
                      file.size("final/en_US/en_US.twitter.txt")/(1024^2)),
           Linecount=c(length(blogs), length(news), length(twitter)),
           Wordcount=c(sum(stri_count_words(blogs)),
                       sum(stri_count_words(news)),
                       sum(stri_count_words(twitter))))
```

##Preprocessing

As can be seen in the table above, the documents are very large and contain a lot of text. For this report, we sample 1% of the data for further analysis. Further pre-processing is then done using the tm package in R, and involves removing punctuation and numbers, converting all to lower-case, removing stopwords (very common words such as "the", "at", "do", etc.) and removing profanity (using the list available at http://www.bannedwordlist.com). Stemming is also performed to count words with a common root together (e.g. "walk", "walks", "walking"). 

```{r, cache=TRUE, echo=FALSE}
options(mc.cores=1)
set.seed(123)
docs_sample <- VCorpus(VectorSource(c(sample(blogs, length(blogs)*0.01),
                                      sample(news, length(news)*0.01),
                                      sample(twitter, length(twitter)*0.01))))
rm(list=c("blogs", "news", "twitter"))
docs_sample_pp <- tm_map(docs_sample, removePunctuation, preserve_intra_word_dashes = TRUE)
for(j in seq(docs_sample_pp)){docs_sample_pp[[j]] <- gsub("-", " ", docs_sample_pp[[j]])}
docs_sample_pp <- tm_map(docs_sample_pp, tolower)
docs_sample_pp <- tm_map(docs_sample_pp, removeWords, stopwords("english"))
docs_sample_pp <- tm_map(docs_sample_pp, removeWords, profanList[,1])
docs_sample_pp <- tm_map(docs_sample_pp, removeNumbers)
docs_sample_pp <- tm_map(docs_sample_pp, stemDocument)
docs_sample_pp <- tm_map(docs_sample_pp, stripWhitespace)
docs_sample_pp <- tm_map(docs_sample_pp, PlainTextDocument)

bf <- docs_sample[[3]]$content
af <- docs_sample_pp[[3]]$content
```

The result is to convert this example:

```{r, echo=FALSE}
bf
```

to this:

```{r, echo=FALSE}
af
```

##N-gram Tokenization

Here, we identify the most common single, double and triples word combinations (words, bigrams and trigrams, respectively). This was done using the RWeka package in R. In calculating the frequencies of the terms, sparse terms were removed to reduce computation time.

```{r, cache=TRUE, echo=FALSE}
library(RWeka)
nGramFunc <- function(n, sparse) {
    options(mc.cores=1)
    Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = n, max = n))
    tdm <- TermDocumentMatrix(docs_sample_pp, control=list(tokenize=Tokenizer))
    termFreq <- as.data.frame(apply(removeSparseTerms(tdm, sparse),1,sum))
    names(termFreq) <- c("Freq")
    termFreq[order(-termFreq$Freq), , drop=FALSE]
}

wordFreq <- nGramFunc(1, 0.99)
bigramFreq <- nGramFunc(2, 0.999)
trigramFreq <- nGramFunc(3, 0.9999)
```

###Word frequencies

Below is a histogram of the 20 most common words in the sample with their frequency as a percentage. For visual effect, a word cloud is also included for the most frequent words.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(wordcloud)
wordPlot <- data.frame(Term=rownames(wordFreq), Frequency=100*(wordFreq$Freq/sum(wordFreq)))
wordPlot$Term <- factor(wordPlot$Term, levels = wordPlot$Term)
ggplot(wordPlot[1:20,], aes(Term, Frequency, fill="red")) +
    geom_bar(stat="identity") +
    theme_classic() +
    theme(text = element_text(size=20), axis.text.x=element_text(angle=45, hjust=1)) +
    ylab("% frequency") +
    guides(fill="none")

wordcloud(rownames(wordFreq),wordFreq$Freq, min.freq=800, colors=brewer.pal(7, "Dark2"))
```

###Bigram frequencies

The 20 most common bigrams (two word combinations):

```{r, echo=FALSE, message=FALSE, warning=FALSE}
bigramPlot <- data.frame(Term=rownames(bigramFreq),
                         Frequency=100*(bigramFreq$Freq/sum(bigramFreq)))
bigramPlot$Term <- factor(bigramPlot$Term, levels = bigramPlot$Term)
ggplot(bigramPlot[1:20,], aes(Term, Frequency, fill="green")) +
    geom_bar(stat="identity") +
    theme_classic() +
    theme(text = element_text(size=20), axis.text.x=element_text(angle=45, hjust=1)) +
    ylab("% frequency") +
    guides(fill="none")
```

###Trigram frequencies

The 20 most common trigrams (three word combinations):

```{r, echo=FALSE, message=FALSE, warning=FALSE}
trigramPlot <- data.frame(Term=rownames(trigramFreq),
                         Frequency=100*(trigramFreq$Freq/sum(trigramFreq)))
trigramPlot$Term <- factor(trigramPlot$Term, levels = trigramPlot$Term)
ggplot(trigramPlot[1:20,], aes(Term, Frequency, fill="blue")) +
    geom_bar(stat="identity") +
    theme_classic() +
    theme(text = element_text(size=20), axis.text.x=element_text(angle=90, hjust=1)) +
    ylab("% frequency") +
    guides(fill="none")
```

Interestingly, celebration phrases (e.g. "happy mothers day", "happy new year", "st patricks day"), feature prominently here.

###Stopword removal

For the analysis so far, stopwords (i.e. very common words such as "the", "at", "do", etc.) were removed during preprocessing. It is clear that these would dominate the word frequencies, and also feature prominently in the most common bigrams and trigrams, so for text analysis it is usual to remove them. There are `r library(tm); length(stopwords("en"))` in total. The figure below shows the cumulative contribution of each unique word (starting from most common) to the total number of words in the sample. To get the "including stopwords" data, the sample was preprocessed in the same way, but without stopword removal.

```{r, cache=TRUE, echo=FALSE}
docs_sample_pp_sw <- tm_map(docs_sample, removePunctuation, preserve_intra_word_dashes = TRUE)
for(j in seq(docs_sample_pp_sw)){docs_sample_pp_sw[[j]] <- gsub("-", " ",
                                                                docs_sample_pp_sw[[j]])}
docs_sample_pp_sw <- tm_map(docs_sample_pp_sw, tolower)
#don't remove the stopwords
docs_sample_pp_sw <- tm_map(docs_sample_pp_sw, removeWords, profanList[,1])
docs_sample_pp_sw <- tm_map(docs_sample_pp_sw, removeNumbers)
docs_sample_pp_sw <- tm_map(docs_sample_pp_sw, stemDocument)
docs_sample_pp_sw <- tm_map(docs_sample_pp_sw, stripWhitespace)
docs_sample_pp_sw <- tm_map(docs_sample_pp_sw, PlainTextDocument)
```

```{r, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library(slam)
tdm <- TermDocumentMatrix(docs_sample_pp)
tdmFreq <- data.frame(Freq=row_sums(tdm))
tdmFreq <- tdmFreq[order(-tdmFreq$Freq), , drop=FALSE]
tdmCum_pc <- 100 * cumsum(tdmFreq) / sum(tdmFreq)

tdm_sw <- TermDocumentMatrix(docs_sample_pp_sw)
tdm_swFreq <- data.frame(Freq=row_sums(tdm_sw))
tdm_swFreq <- tdm_swFreq[order(-tdm_swFreq$Freq), , drop=FALSE]
tdm_swCum_pc <- 100 * cumsum(tdm_swFreq) / sum(tdm_swFreq)

plot(tdmCum_pc$Freq, type="l", xlab="Number of terms", ylab="% of all words",
     log="x", lwd=3, col="green")
lines(tdm_swCum_pc, col="blue", lwd=3)
legend(1, 90, c("without stopwords", "including stopwords"),
       lty=c(1,1), lwd=c(3,3), col=c("green","blue"))
```

Removing the stopwords, we see that `r max(which(tdmCum_pc$Freq<=50))` unique words accounts for 50% of all the words, while this figure is `r max(which(tdm_swCum_pc$Freq<=50))` if stopwords are left in. Just the three most common words "the", "and", "for" account for more than 10% of all words in the sample. As these words are so common, it would not make sense to ignore them when building a predictive text model, and so these will be left in. `r max(which(tdm_swCum_pc$Freq<=90))` unique words (including stopwords) are required to cover 90% of the sample.

##Next steps

Following this exploratory data analysis, the next steps will be to take these findings and apply them to a basic n-gram model for predictive text. It will be important to find ways of maximizing the accuracy of the model while reducing the memory requirements and the runtime.